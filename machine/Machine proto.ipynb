{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d837f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "train_data = pd.read_table('reddit_data.txt',encoding='latin_1')\n",
    "train_data.drop_duplicates(subset=['title'], inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "for x in range(len(train_data)):\n",
    "    train_data['title'].iloc[x]=re.sub(r'[^a-zA-Z ]', '',train_data['title'].iloc[x])\n",
    "train_data['title'] = train_data['title'].str.replace('^ +', \"\")\n",
    "train_data['title'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "X_train = []\n",
    "for sentence in tqdm(train_data['title']):\n",
    "    tokenized_sentence = word_tokenize(sentence) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stop_words] # 불용어 제거\n",
    "    X_train.append(stopwords_removed_sentence)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index)\n",
    "rare_cnt = 0\n",
    "total_freq = 0\n",
    "rare_freq = 0\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "y_train = np.array(train_data['label'],dtype=np.float64)\n",
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print('리뷰의 최대 길이 :',max(len(l) for l in X_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, X_train))/len(X_train))\n",
    "plt.hist([len(s) for s in X_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "  count = 0\n",
    "  for sentence in nested_list:\n",
    "    if(len(sentence) <= max_len):\n",
    "        count = count + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nested_list))*100))\n",
    "max_len = 190\n",
    "below_threshold_len(max_len, X_train)\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import h5py\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
