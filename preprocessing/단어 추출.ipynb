{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5db27fdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_26376/1739268815.py:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  c['title'] = c['title'].str.replace('^ +', \"\")\n",
      "\n",
      "  0%|                                                                                         | 0/3598 [00:00<?, ?it/s]\u001b[A\n",
      " 23%|█████████████████▌                                                           | 823/3598 [00:00<00:00, 8170.34it/s]\u001b[A\n",
      " 46%|██████████████████████████████████▋                                         | 1641/3598 [00:00<00:00, 8093.97it/s]\u001b[A\n",
      " 68%|███████████████████████████████████████████████████▊                        | 2451/3598 [00:00<00:00, 7927.08it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 3598/3598 [00:00<00:00, 7500.26it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 7920\n",
      "등장 빈도가 2번 이하인 희귀 단어의 수: 5513\n",
      "단어 집합에서 희귀 단어의 비율: 69.60858585858585\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 14.341697880355945\n",
      "5513\n",
      "2408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## from nltk import Text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords  \n",
    "import urllib.request\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "train_data = pd.read_table('title_output.txt',encoding='ANSI')\n",
    "train_data['title_vader']=train_data['title_vader'].apply(pd.to_numeric,errors='coerce').fillna(2)\n",
    "c=pd.DataFrame({'title':[]})\n",
    "i=0\n",
    "for x in range(len(train_data)):\n",
    "    if 0.8<float(train_data['title_vader'][x])<=1.0:\n",
    "        c.loc[i]=train_data['title'][x]\n",
    "        i=i+1\n",
    "c['title'].nunique()\n",
    "c.drop_duplicates(subset=['title'], inplace=True)\n",
    "for x in range(len(c)):\n",
    "    c['title'].iloc[x]=re.sub(r'[^a-zA-Z ]', '',str(c['title'].iloc[x]))\n",
    "c['title'] = c['title'].str.replace('^ +', \"\")\n",
    "c['title'].replace('', np.nan, inplace=True)\n",
    "stop_words = set(stopwords.words('english')) \n",
    "X_train = []\n",
    "for sentence in tqdm(c['title'].str.lower()):\n",
    "    tokenized_sentence = word_tokenize(str(sentence)) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stop_words] # 불용어 제거\n",
    "    X_train.append(stopwords_removed_sentence)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "print(rare_cnt)\n",
    "print(vocab_size)\n",
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "29fc880a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bitcoin',\n",
       " 'buy',\n",
       " 'free',\n",
       " 'btc',\n",
       " 'best',\n",
       " 'money',\n",
       " 'get',\n",
       " 'crypto',\n",
       " 'like',\n",
       " 'good',\n",
       " 'time',\n",
       " 'help',\n",
       " 'moon',\n",
       " 'new',\n",
       " 'way',\n",
       " 'great',\n",
       " 'make',\n",
       " 'k',\n",
       " 'earn',\n",
       " 'one',\n",
       " 'want',\n",
       " 'amp',\n",
       " 'please',\n",
       " 'would',\n",
       " 'people',\n",
       " 'wallet',\n",
       " 'mining',\n",
       " 'use',\n",
       " 'know',\n",
       " 'link',\n",
       " 'need',\n",
       " 'price',\n",
       " 'bitcoins',\n",
       " 'easy',\n",
       " 'us',\n",
       " 'love',\n",
       " 'bullish',\n",
       " 'join',\n",
       " 'today',\n",
       " 'im',\n",
       " 'guys',\n",
       " 'card',\n",
       " 'go',\n",
       " 'value',\n",
       " 'anyone',\n",
       " 'start',\n",
       " 'first',\n",
       " 'bull',\n",
       " 'trading',\n",
       " 'app',\n",
       " 'market',\n",
       " 'cryptocurrency',\n",
       " 'day',\n",
       " 'long',\n",
       " 'dont',\n",
       " 'going',\n",
       " 'profit',\n",
       " 'invest',\n",
       " 'dip',\n",
       " 'see',\n",
       " 'support',\n",
       " 'back',\n",
       " 'think',\n",
       " 'win',\n",
       " 'everyone',\n",
       " 'made',\n",
       " 'platform',\n",
       " 'investment',\n",
       " 'safe',\n",
       " 'using',\n",
       " 'place',\n",
       " 'much',\n",
       " 'trade',\n",
       " 'world',\n",
       " 'coin',\n",
       " 'friends',\n",
       " 'share',\n",
       " 'thanks',\n",
       " 'worth',\n",
       " 'better',\n",
       " 'every',\n",
       " 'future',\n",
       " 'browser',\n",
       " 'cash',\n",
       " 'still',\n",
       " 'call',\n",
       " 'years',\n",
       " 'happy',\n",
       " 'gift',\n",
       " 'right',\n",
       " 'account',\n",
       " 'got',\n",
       " 'last',\n",
       " 'opportunity',\n",
       " 'super',\n",
       " 'year',\n",
       " 'really',\n",
       " 'real',\n",
       " 'run',\n",
       " 'looking',\n",
       " 'thank',\n",
       " 'could',\n",
       " 'also',\n",
       " 'let',\n",
       " 'days',\n",
       " 'take',\n",
       " 'exchange',\n",
       " 'well',\n",
       " 'community',\n",
       " 'without',\n",
       " 'hold',\n",
       " 'hope',\n",
       " 'rich',\n",
       " 'work',\n",
       " 'video',\n",
       " 'amazing',\n",
       " 'buying',\n",
       " 'check',\n",
       " 'trust',\n",
       " 'credit',\n",
       " 'next',\n",
       " 'huge',\n",
       " 'top',\n",
       " 'even',\n",
       " 'find',\n",
       " 'week',\n",
       " 'blockchain',\n",
       " 'news',\n",
       " 'many',\n",
       " 'interested',\n",
       " 'ready',\n",
       " 'hodl',\n",
       " 'found',\n",
       " 'keep',\n",
       " 'send',\n",
       " 'making',\n",
       " 'store',\n",
       " 'never',\n",
       " 'rewards',\n",
       " 'online',\n",
       " 'chance',\n",
       " 'secure',\n",
       " 'give',\n",
       " 'click',\n",
       " 'enjoy',\n",
       " 'network',\n",
       " 'ever',\n",
       " 'someone',\n",
       " 'energy',\n",
       " 'gold']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.word_index)[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0fa0c0d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1125"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04651f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
